# Note/disclosure: This file has been (partially or fully) generated by an AI agent.
import time
from datetime import datetime, timezone

import boto3
from localstack.aws.connect import connect_to
from localstack.utils.strings import short_uid
from localstack.utils.sync import retry

from aws_proxy.shared.models import ProxyConfig


# =============================================================================
# CloudWatch Metrics Tests
# =============================================================================


def test_cloudwatch_metric_operations(start_aws_proxy, cleanups):
    """Test basic CloudWatch metric operations with proxy.

    Note: CloudWatch metrics have significant eventual consistency delays (2-5 minutes).
    This test focuses on verifying the proxy functionality by testing PutMetricData
    and ListMetrics operations rather than waiting for GetMetricData to return values.
    """
    namespace = f"TestNamespace/{short_uid()}"
    metric_name = f"TestMetric-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch metrics
    config = ProxyConfig(services={"cloudwatch": {"resources": [".*"]}})
    start_aws_proxy(config)

    # create clients
    cw_client = connect_to().cloudwatch
    cw_client_aws = boto3.client("cloudwatch")

    # put metric data to AWS directly
    timestamp = datetime.now(timezone.utc)
    cw_client_aws.put_metric_data(
        Namespace=namespace,
        MetricData=[
            {
                "MetricName": metric_name,
                "Value": 42.0,
                "Unit": "Count",
                "Timestamp": timestamp,
            }
        ],
    )

    # put metric data through local client (proxied) to verify proxy forwards correctly
    metric_name_2 = f"TestMetric2-{short_uid()}"
    cw_client.put_metric_data(
        Namespace=namespace,
        MetricData=[
            {
                "MetricName": metric_name_2,
                "Value": 100.0,
                "Unit": "Count",
                "Timestamp": datetime.now(timezone.utc),
            }
        ],
    )

    # verify metrics exist via list_metrics (faster than get_metric_data for new metrics)
    def _verify_metrics_aws():
        response = cw_client_aws.list_metrics(Namespace=namespace)
        metric_names = [m["MetricName"] for m in response.get("Metrics", [])]
        if metric_name not in metric_names or metric_name_2 not in metric_names:
            raise AssertionError(f"Metrics not found in AWS yet. Found: {metric_names}")
        return response

    metrics_aws = retry(_verify_metrics_aws, retries=20, sleep=5)
    metric_names = [m["MetricName"] for m in metrics_aws["Metrics"]]
    assert metric_name in metric_names
    assert metric_name_2 in metric_names  # This proves the proxy forwarded the request

    # verify list_metrics through proxy returns the same data
    metrics_local = cw_client.list_metrics(Namespace=namespace)
    metric_names_local = [m["MetricName"] for m in metrics_local.get("Metrics", [])]
    assert metric_name in metric_names_local
    assert metric_name_2 in metric_names_local


def test_cloudwatch_alarm_operations(start_aws_proxy, cleanups):
    """Test CloudWatch alarm operations with proxy."""
    test_id = short_uid()
    alarm_name = f"test-alarm-{test_id}"
    namespace = f"TestNamespace/{short_uid()}"
    metric_name = f"TestMetric-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch alarms matching "test-alarm-{test_id}*"
    config = ProxyConfig(
        services={"cloudwatch": {"resources": [f".*:alarm:test-alarm-{test_id}.*"]}}
    )
    start_aws_proxy(config)

    # create clients
    cw_client = connect_to().cloudwatch
    cw_client_aws = boto3.client("cloudwatch")

    # create alarm in AWS
    cw_client_aws.put_metric_alarm(
        AlarmName=alarm_name,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Average",
        Period=60,
        EvaluationPeriods=1,
        Threshold=50.0,
        ComparisonOperator="GreaterThanThreshold",
    )
    cleanups.append(lambda: cw_client_aws.delete_alarms(AlarmNames=[alarm_name]))

    # describe alarm through local client (proxied)
    alarms_local = cw_client.describe_alarms(AlarmNames=[alarm_name])
    alarms_aws = cw_client_aws.describe_alarms(AlarmNames=[alarm_name])

    assert len(alarms_local["MetricAlarms"]) == 1
    assert len(alarms_aws["MetricAlarms"]) == 1
    assert alarms_local["MetricAlarms"][0]["AlarmName"] == alarm_name
    assert (
        alarms_local["MetricAlarms"][0]["AlarmArn"]
        == alarms_aws["MetricAlarms"][0]["AlarmArn"]
    )

    # create alarm through local client (proxied)
    alarm_name_2 = f"test-alarm-{test_id}-2"
    cw_client.put_metric_alarm(
        AlarmName=alarm_name_2,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Sum",
        Period=300,
        EvaluationPeriods=2,
        Threshold=100.0,
        ComparisonOperator="LessThanThreshold",
    )
    cleanups.append(lambda: cw_client_aws.delete_alarms(AlarmNames=[alarm_name_2]))

    # verify alarm exists in AWS
    alarms_aws_2 = cw_client_aws.describe_alarms(AlarmNames=[alarm_name_2])
    assert len(alarms_aws_2["MetricAlarms"]) == 1
    assert alarms_aws_2["MetricAlarms"][0]["AlarmName"] == alarm_name_2


def test_cloudwatch_readonly_operations(start_aws_proxy, cleanups):
    """Test CloudWatch operations in read-only proxy mode."""
    alarm_name = f"test-readonly-alarm-{short_uid()}"
    namespace = f"TestNamespace/{short_uid()}"
    metric_name = f"TestMetric-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch in read-only mode
    config = ProxyConfig(
        services={
            "cloudwatch": {"resources": [f".*:alarm:{alarm_name}"], "read_only": True}
        }
    )
    start_aws_proxy(config)

    # create clients
    cw_client = connect_to().cloudwatch
    cw_client_aws = boto3.client("cloudwatch")

    # create alarm in AWS (this should succeed as it's direct AWS client)
    cw_client_aws.put_metric_alarm(
        AlarmName=alarm_name,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Average",
        Period=60,
        EvaluationPeriods=1,
        Threshold=50.0,
        ComparisonOperator="GreaterThanThreshold",
    )
    cleanups.append(lambda: cw_client_aws.delete_alarms(AlarmNames=[alarm_name]))

    # assert that local call for describe_alarms is proxied and results are consistent
    alarms_local = cw_client.describe_alarms(AlarmNames=[alarm_name])
    alarms_aws = cw_client_aws.describe_alarms(AlarmNames=[alarm_name])
    assert (
        alarms_local["MetricAlarms"][0]["AlarmArn"]
        == alarms_aws["MetricAlarms"][0]["AlarmArn"]
    )

    # Negative test: attempt write operations with proxied client in read-only mode
    # Create a new alarm using the proxied client (should succeed in LocalStack locally)
    new_alarm_name = f"no-proxy-alarm-{short_uid()}"
    cw_client.put_metric_alarm(
        AlarmName=new_alarm_name,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Average",
        Period=60,
        EvaluationPeriods=1,
        Threshold=50.0,
        ComparisonOperator="GreaterThanThreshold",
    )
    cleanups.append(lambda: cw_client.delete_alarms(AlarmNames=[new_alarm_name]))

    # Verify that this new alarm does NOT exist in real AWS
    alarms_aws_new = cw_client_aws.describe_alarms(AlarmNames=[new_alarm_name])
    assert len(alarms_aws_new["MetricAlarms"]) == 0


def test_cloudwatch_resource_name_matching(start_aws_proxy, cleanups):
    """Test that proxy forwards requests for specific CloudWatch alarms matching ARN pattern."""
    alarm_name_match = f"proxy-alarm-{short_uid()}"
    alarm_name_nomatch = f"local-alarm-{short_uid()}"
    namespace = f"TestNamespace/{short_uid()}"
    metric_name = f"TestMetric-{short_uid()}"

    # start proxy - only forwarding requests for alarms starting with "proxy-"
    config = ProxyConfig(services={"cloudwatch": {"resources": ".*:alarm:proxy-.*"}})
    start_aws_proxy(config)

    # create clients
    cw_client = connect_to().cloudwatch
    cw_client_aws = boto3.client("cloudwatch")

    # create alarm in AWS that matches the pattern
    cw_client_aws.put_metric_alarm(
        AlarmName=alarm_name_match,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Average",
        Period=60,
        EvaluationPeriods=1,
        Threshold=50.0,
        ComparisonOperator="GreaterThanThreshold",
    )
    cleanups.append(lambda: cw_client_aws.delete_alarms(AlarmNames=[alarm_name_match]))

    # assert that the matching alarm is proxied
    alarms_local = cw_client.describe_alarms(AlarmNames=[alarm_name_match])
    alarms_aws = cw_client_aws.describe_alarms(AlarmNames=[alarm_name_match])
    assert len(alarms_local["MetricAlarms"]) == 1
    assert (
        alarms_local["MetricAlarms"][0]["AlarmArn"]
        == alarms_aws["MetricAlarms"][0]["AlarmArn"]
    )

    # create a local alarm that doesn't match the pattern
    cw_client.put_metric_alarm(
        AlarmName=alarm_name_nomatch,
        MetricName=metric_name,
        Namespace=namespace,
        Statistic="Average",
        Period=60,
        EvaluationPeriods=1,
        Threshold=50.0,
        ComparisonOperator="GreaterThanThreshold",
    )
    cleanups.append(lambda: cw_client.delete_alarms(AlarmNames=[alarm_name_nomatch]))

    # verify that the non-matching alarm was created locally but NOT in AWS
    alarms_aws_nomatch = cw_client_aws.describe_alarms(AlarmNames=[alarm_name_nomatch])
    assert len(alarms_aws_nomatch["MetricAlarms"]) == 0


# =============================================================================
# CloudWatch Logs Tests
# =============================================================================


def test_logs_group_operations(start_aws_proxy, cleanups):
    """Test basic CloudWatch Logs group operations with proxy."""
    log_group_name = f"/test/logs/{short_uid()}"

    # start proxy - forwarding requests for CloudWatch Logs
    config = ProxyConfig(
        services={"logs": {"resources": [f".*:log-group:{log_group_name}:.*"]}}
    )
    start_aws_proxy(config)

    # create clients
    logs_client = connect_to().logs
    logs_client_aws = boto3.client("logs")

    # create log group in AWS
    logs_client_aws.create_log_group(logGroupName=log_group_name)
    cleanups.append(
        lambda: logs_client_aws.delete_log_group(logGroupName=log_group_name)
    )

    # describe log groups through local client (proxied)
    groups_local = logs_client.describe_log_groups(logGroupNamePrefix=log_group_name)
    groups_aws = logs_client_aws.describe_log_groups(logGroupNamePrefix=log_group_name)

    assert len(groups_local["logGroups"]) == 1
    assert len(groups_aws["logGroups"]) == 1
    assert groups_local["logGroups"][0]["logGroupName"] == log_group_name
    assert groups_local["logGroups"][0]["arn"] == groups_aws["logGroups"][0]["arn"]


def test_logs_stream_and_events(start_aws_proxy, cleanups):
    """Test CloudWatch Logs stream and event operations with proxy."""
    log_group_name = f"/test/logs/{short_uid()}"
    log_stream_name = f"test-stream-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch Logs
    config = ProxyConfig(
        services={"logs": {"resources": [f".*:log-group:{log_group_name}:.*"]}}
    )
    start_aws_proxy(config)

    # create clients
    logs_client = connect_to().logs
    logs_client_aws = boto3.client("logs")

    # create log group and stream in AWS
    logs_client_aws.create_log_group(logGroupName=log_group_name)
    cleanups.append(
        lambda: logs_client_aws.delete_log_group(logGroupName=log_group_name)
    )

    logs_client_aws.create_log_stream(
        logGroupName=log_group_name, logStreamName=log_stream_name
    )

    # put log events through AWS client
    timestamp = int(time.time() * 1000)
    logs_client_aws.put_log_events(
        logGroupName=log_group_name,
        logStreamName=log_stream_name,
        logEvents=[
            {"timestamp": timestamp, "message": "Test message from AWS"},
        ],
    )

    # get log events through local client (proxied)
    def _get_log_events():
        response = logs_client.get_log_events(
            logGroupName=log_group_name,
            logStreamName=log_stream_name,
            startFromHead=True,
        )
        if not response["events"]:
            raise AssertionError("Log events not available yet")
        return response

    events_local = retry(_get_log_events, retries=10, sleep=2)
    assert len(events_local["events"]) >= 1
    assert events_local["events"][0]["message"] == "Test message from AWS"

    # put log events through local client (proxied)
    timestamp_2 = int(time.time() * 1000)
    logs_client.put_log_events(
        logGroupName=log_group_name,
        logStreamName=log_stream_name,
        logEvents=[
            {"timestamp": timestamp_2, "message": "Test message from LocalStack"},
        ],
    )

    # verify via AWS client
    def _verify_events_aws():
        response = logs_client_aws.get_log_events(
            logGroupName=log_group_name,
            logStreamName=log_stream_name,
            startFromHead=True,
        )
        messages = [e["message"] for e in response["events"]]
        if "Test message from LocalStack" not in messages:
            raise AssertionError("Log event from LocalStack not found in AWS")
        return response

    events_aws = retry(_verify_events_aws, retries=10, sleep=2)
    messages = [e["message"] for e in events_aws["events"]]
    assert "Test message from AWS" in messages
    assert "Test message from LocalStack" in messages


def test_logs_readonly_operations(start_aws_proxy, cleanups):
    """Test CloudWatch Logs operations in read-only proxy mode."""
    log_group_name = f"/test/readonly/{short_uid()}"
    log_stream_name = f"test-stream-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch Logs in read-only mode
    config = ProxyConfig(
        services={
            "logs": {
                "resources": [f".*:log-group:{log_group_name}:.*"],
                "read_only": True,
            }
        }
    )
    start_aws_proxy(config)

    # create clients
    logs_client = connect_to().logs
    logs_client_aws = boto3.client("logs")

    # create log group and stream in AWS (this should succeed as it's direct AWS client)
    logs_client_aws.create_log_group(logGroupName=log_group_name)
    cleanups.append(
        lambda: logs_client_aws.delete_log_group(logGroupName=log_group_name)
    )

    logs_client_aws.create_log_stream(
        logGroupName=log_group_name, logStreamName=log_stream_name
    )

    # put log events through AWS client
    timestamp = int(time.time() * 1000)
    logs_client_aws.put_log_events(
        logGroupName=log_group_name,
        logStreamName=log_stream_name,
        logEvents=[
            {"timestamp": timestamp, "message": "Test message from AWS"},
        ],
    )

    # assert that local call for describe_log_groups is proxied and results are consistent
    groups_local = logs_client.describe_log_groups(logGroupNamePrefix=log_group_name)
    groups_aws = logs_client_aws.describe_log_groups(logGroupNamePrefix=log_group_name)
    assert groups_local["logGroups"][0]["arn"] == groups_aws["logGroups"][0]["arn"]

    # get log events through local client (proxied) - should work in read-only mode
    def _get_log_events():
        response = logs_client.get_log_events(
            logGroupName=log_group_name,
            logStreamName=log_stream_name,
            startFromHead=True,
        )
        if not response["events"]:
            raise AssertionError("Log events not available yet")
        return response

    events_local = retry(_get_log_events, retries=10, sleep=2)
    assert events_local["events"][0]["message"] == "Test message from AWS"

    # Negative test: attempt write operations with proxied client in read-only mode
    # Create a new log group using the proxied client (should succeed in LocalStack locally)
    new_log_group_name = f"/test/no-proxy/{short_uid()}"
    logs_client.create_log_group(logGroupName=new_log_group_name)
    cleanups.append(
        lambda: logs_client.delete_log_group(logGroupName=new_log_group_name)
    )

    # Verify that this new log group does NOT exist in real AWS
    groups_aws_new = logs_client_aws.describe_log_groups(
        logGroupNamePrefix=new_log_group_name
    )
    assert len(groups_aws_new["logGroups"]) == 0


def test_logs_resource_name_matching(start_aws_proxy, cleanups):
    """Test that proxy forwards requests for specific log groups matching ARN pattern."""
    log_group_match = f"/proxy/logs/{short_uid()}"
    log_group_nomatch = f"/local/logs/{short_uid()}"

    # start proxy - only forwarding requests for log groups starting with "/proxy/"
    config = ProxyConfig(services={"logs": {"resources": ".*:log-group:/proxy/.*"}})
    start_aws_proxy(config)

    # create clients
    logs_client = connect_to().logs
    logs_client_aws = boto3.client("logs")

    # create log group in AWS that matches the pattern
    logs_client_aws.create_log_group(logGroupName=log_group_match)
    cleanups.append(
        lambda: logs_client_aws.delete_log_group(logGroupName=log_group_match)
    )

    # assert that the matching log group is proxied
    groups_local = logs_client.describe_log_groups(logGroupNamePrefix=log_group_match)
    groups_aws = logs_client_aws.describe_log_groups(logGroupNamePrefix=log_group_match)
    assert len(groups_local["logGroups"]) == 1
    assert groups_local["logGroups"][0]["arn"] == groups_aws["logGroups"][0]["arn"]

    # create a local log group that doesn't match the pattern
    logs_client.create_log_group(logGroupName=log_group_nomatch)
    cleanups.append(
        lambda: logs_client.delete_log_group(logGroupName=log_group_nomatch)
    )

    # verify that the non-matching log group was created locally but NOT in AWS
    groups_aws_nomatch = logs_client_aws.describe_log_groups(
        logGroupNamePrefix=log_group_nomatch
    )
    assert len(groups_aws_nomatch["logGroups"]) == 0


def test_logs_filter_log_events(start_aws_proxy, cleanups):
    """Test CloudWatch Logs filter_log_events operation with proxy."""
    log_group_name = f"/test/filter/{short_uid()}"
    log_stream_name_1 = f"stream-1-{short_uid()}"
    log_stream_name_2 = f"stream-2-{short_uid()}"

    # start proxy - forwarding requests for CloudWatch Logs
    config = ProxyConfig(
        services={"logs": {"resources": [f".*:log-group:{log_group_name}:.*"]}}
    )
    start_aws_proxy(config)

    # create clients
    logs_client = connect_to().logs
    logs_client_aws = boto3.client("logs")

    # create log group and streams in AWS
    logs_client_aws.create_log_group(logGroupName=log_group_name)
    cleanups.append(
        lambda: logs_client_aws.delete_log_group(logGroupName=log_group_name)
    )

    logs_client_aws.create_log_stream(
        logGroupName=log_group_name, logStreamName=log_stream_name_1
    )
    logs_client_aws.create_log_stream(
        logGroupName=log_group_name, logStreamName=log_stream_name_2
    )

    # put log events with different messages
    timestamp = int(time.time() * 1000)
    logs_client_aws.put_log_events(
        logGroupName=log_group_name,
        logStreamName=log_stream_name_1,
        logEvents=[
            {"timestamp": timestamp, "message": "ERROR: Something went wrong"},
            {"timestamp": timestamp + 1, "message": "INFO: Normal operation"},
        ],
    )
    logs_client_aws.put_log_events(
        logGroupName=log_group_name,
        logStreamName=log_stream_name_2,
        logEvents=[
            {"timestamp": timestamp + 2, "message": "ERROR: Another error"},
            {"timestamp": timestamp + 3, "message": "DEBUG: Debug info"},
        ],
    )

    # filter log events through local client (proxied)
    def _filter_log_events():
        response = logs_client.filter_log_events(
            logGroupName=log_group_name,
            filterPattern="ERROR",
        )
        if len(response["events"]) < 2:
            raise AssertionError("Not all error events found yet")
        return response

    filtered_local = retry(_filter_log_events, retries=15, sleep=2)

    # verify only ERROR messages are returned
    assert len(filtered_local["events"]) >= 2
    for event in filtered_local["events"]:
        assert "ERROR" in event["message"]

    # compare with AWS client results
    filtered_aws = logs_client_aws.filter_log_events(
        logGroupName=log_group_name,
        filterPattern="ERROR",
    )
    assert len(filtered_local["events"]) == len(filtered_aws["events"])
